import numpy as np
import matplotlib.pyplot as plt

# Input / output data
x = np.array([0.03, 0.19, 0.34, 0.46, 0.78, 0.81, 1.08, 1.18, 1.39, 1.60, 1.65, 1.90])
y = np.array([0.67, 0.85, 1.05, 1.0, 1.40, 1.5, 1.3, 1.54, 1.55, 1.68, 1.73, 1.6 ])

# Linear regression model
def f(x, phi0, phi1):
    return phi0 + phi1 * x

# Plotting helper
def plot(x, y, phi0, phi1):
    fig,ax = plt.subplots()
    ax.scatter(x,y)
    plt.xlim([0,2.0])
    plt.ylim([0,2.0])
    ax.set_xlabel('Input, $x$')
    ax.set_ylabel('Output, $y$')
    x_line = np.arange(0,2,0.01)
    y_line = f(x_line, phi0, phi1)
    plt.plot(x_line, y_line,'b-',lw=2)
    plt.show()

# Mean Squared Error loss
def compute_loss(x,y,phi0,phi1):
    y_pred = f(x, phi0, phi1)
    return np.mean((y - y_pred)**2)

# Gradient computation
def compute_gradients(x, y, phi0, phi1):
    n = len(x)
    y_pred = f(x, phi0, phi1)
    dphi0 = (-2/n) * np.sum(y - y_pred)
    dphi1 = (-2/n) * np.sum(x * (y - y_pred))
    return dphi0, dphi1

# Gradient Descent
def gradient_descent(x, y, lr=0.1, epochs=1000):
    phi0, phi1 = 0.0, 0.0  # start
    history = []
    for i in range(epochs):
        dphi0, dphi1 = compute_gradients(x, y, phi0, phi1)
        phi0 -= lr * dphi0
        phi1 -= lr * dphi1
        loss = compute_loss(x, y, phi0, phi1)
        history.append(loss)
        if i % 100 == 0:
            print(f"Epoch {i}: Loss={loss:.4f}, phi0={phi0:.3f}, phi1={phi1:.3f}")
    return phi0, phi1, history

# Run gradient descent
phi0, phi1, history = gradient_descent(x, y, lr=0.1, epochs=1000)

print(f"\nBest fit: phi0={phi0:.3f}, phi1={phi1:.3f}, Loss={compute_loss(x,y,phi0,phi1):.4f}")
plot(x, y, phi0, phi1)

# Plot loss curve
plt.plot(history)
plt.xlabel("Epochs")
plt.ylabel("Loss (MSE)")
plt.title("Training Loss Curve")
plt.show()

