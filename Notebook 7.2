import numpy as np
import matplotlib.pyplot as plt

# ---------------------------------------------------
# 1. Network setup
# ---------------------------------------------------
np.random.seed(0)

K = 5          # hidden layers
D = 6          # neurons per hidden layer
D_i = 1        # input dimension
D_o = 1        # output dimension

all_weights = [None]*(K+1)
all_biases  = [None]*(K+1)

# Input & output layers
all_weights[0] = np.random.normal(size=(D, D_i))
all_weights[-1] = np.random.normal(size=(D_o, D))
all_biases[0] = np.random.normal(size=(D,1))
all_biases[-1] = np.random.normal(size=(D_o,1))

# Hidden layers
for layer in range(1, K):
    all_weights[layer] = np.random.normal(size=(D,D))
    all_biases[layer] = np.random.normal(size=(D,1))

# ---------------------------------------------------
# 2. Activation functions
# ---------------------------------------------------
def ReLU(preactivation):
    return preactivation.clip(0.0)

def indicator_function(x):
    x_in = np.array(x)
    x_in[x_in>0] = 1
    x_in[x_in<=0] = 0
    return x_in

# ---------------------------------------------------
# 3. Forward pass
# ---------------------------------------------------
def compute_network_output(net_input, all_weights, all_biases):
    K = len(all_weights)-1
    all_f = [None]*(K+1)
    all_h = [None]*(K+1)
    all_h[0] = net_input

    # hidden layers
    for layer in range(K):
        all_f[layer] = np.matmul(all_weights[layer], all_h[layer]) + all_biases[layer]
        all_h[layer+1] = ReLU(all_f[layer])

    # output layer
    all_f[K] = np.matmul(all_weights[K], all_h[K]) + all_biases[K]
    net_output = all_f[K]
    return net_output, all_f, all_h

# ---------------------------------------------------
# 4. Loss functions
# ---------------------------------------------------
def least_squares_loss(net_output, y):
    return np.sum((net_output - y)**2)

def d_loss_d_output(net_output, y):
    return 2*(net_output - y)

# ---------------------------------------------------
# 5. Test forward pass
# ---------------------------------------------------
net_input = np.ones((D_i,1))*1.2
net_output, all_f, all_h = compute_network_output(net_input, all_weights, all_biases)
print("True output = 1.907, Your answer = %3.3f"%(net_output[0,0]))

# ---------------------------------------------------
# 6. Backward pass
# ---------------------------------------------------
def backward_pass(all_weights, all_biases, all_f, all_h, y):
    K = len(all_weights)-1
    all_dl_dweights = [None]*(K+1)
    all_dl_dbiases  = [None]*(K+1)
    all_dl_df = [None]*(K+1)
    all_dl_dh = [None]*(K+1)

    # derivative of loss w.r.t final preactivation
    all_dl_df[K] = np.array(d_loss_d_output(all_f[K], y))

    for layer in range(K, -1, -1):
        # dL/db = dL/df
        all_dl_dbiases[layer] = np.array(all_dl_df[layer])

        # dL/dW = dL/df * h^T
        all_dl_dweights[layer] = np.matmul(all_dl_df[layer], all_h[layer].T)

        # dL/dh (for previous layer) = W^T * dL/df
        all_dl_dh[layer] = np.matmul(all_weights[layer].T, all_dl_df[layer])

        if layer > 0:
            # dL/df_prev = dL/dh_prev ⊙ I(f_prev>0)
            all_dl_df[layer-1] = all_dl_dh[layer] * indicator_function(all_f[layer-1])

    return all_dl_dweights, all_dl_dbiases

# ---------------------------------------------------
# 7. Run backward pass
# ---------------------------------------------------
y = np.ones((D_o,1))*20.0
all_dl_dweights, all_dl_dbiases = backward_pass(all_weights, all_biases, all_f, all_h, y)

# ---------------------------------------------------
# 8. Verify with finite differences
# ---------------------------------------------------
np.set_printoptions(precision=3)
delta_fd = 1e-6
all_dl_dweights_fd = [None]*(K+1)
all_dl_dbiases_fd  = [None]*(K+1)

# --- Bias check ---
for layer in range(K+1):
    dl_dbias = np.zeros_like(all_dl_dbiases[layer])
    for row in range(all_biases[layer].shape[0]):
        all_biases_copy = [np.array(x) for x in all_biases]
        all_biases_copy[layer][row] += delta_fd
        out1, *_ = compute_network_output(net_input, all_weights, all_biases_copy)
        out2, *_ = compute_network_output(net_input, all_weights, all_biases)
        dl_dbias[row] = (least_squares_loss(out1, y) - least_squares_loss(out2, y))/delta_fd
    all_dl_dbiases_fd[layer] = np.array(dl_dbias)

    print("-----------------------------------------------")
    print("Bias %d, derivatives from backprop:"%(layer))
    print(all_dl_dbiases[layer])
    print("Bias %d, derivatives from finite differences"%(layer))
    print(all_dl_dbiases_fd[layer])
    if np.allclose(all_dl_dbiases_fd[layer], all_dl_dbiases[layer], rtol=1e-5, atol=1e-8):
        print("✅ Success! Derivatives match.")
    else:
        print("❌ Failure! Derivatives differ.")

# --- Weights check ---
for layer in range(K+1):
    dl_dweight = np.zeros_like(all_dl_dweights[layer])
    for row in range(all_weights[layer].shape[0]):
        for col in range(all_weights[layer].shape[1]):
            all_weights_copy = [np.array(x) for x in all_weights]
            all_weights_copy[layer][row][col] += delta_fd
            out1, *_ = compute_network_output(net_input, all_weights_copy, all_biases)
            out2, *_ = compute_network_output(net_input, all_weights, all_biases)
            dl_dweight[row][col] = (least_squares_loss(out1, y) - least_squares_loss(out2, y))/delta_fd
    all_dl_dweights_fd[layer] = np.array(dl_dweight)

    print("-----------------------------------------------")
    print("Weight %d, derivatives from backprop:"%(layer))
    print(all_dl_dweights[layer])
    print("Weight %d, derivatives from finite differences"%(layer))
    print(all_dl_dweights_fd[layer])
    if np.allclose(all_dl_dweights_fd[layer], all_dl_dweights[layer], rtol=1e-5, atol=1e-8):
        print("✅ Success! Derivatives match.")
    else:
        print("❌ Failure! Derivatives differ.")
