# ✅ Notebook 7.3: Initialization — Complete Solution

import numpy as np
import matplotlib.pyplot as plt

# -----------------------------
# Helper functions
# -----------------------------
def init_params(K, D, sigma_sq_omega):
    np.random.seed(0)
    D_i, D_o = 1, 1
    all_weights = [None] * (K + 1)
    all_biases = [None] * (K + 1)
    all_weights[0] = np.random.normal(size=(D, D_i)) * np.sqrt(sigma_sq_omega)
    all_biases[0] = np.zeros((D, 1))
    for layer in range(1, K):
        all_weights[layer] = np.random.normal(size=(D, D)) * np.sqrt(sigma_sq_omega)
        all_biases[layer] = np.zeros((D, 1))
    all_weights[-1] = np.random.normal(size=(D_o, D)) * np.sqrt(sigma_sq_omega)
    all_biases[-1] = np.zeros((D_o, 1))
    return all_weights, all_biases


def ReLU(preactivation):
    return preactivation.clip(0.0)


def compute_network_output(net_input, all_weights, all_biases):
    K = len(all_weights) - 1
    all_f, all_h = [None]*(K+1), [None]*(K+1)
    all_h[0] = net_input
    for layer in range(K):
        all_f[layer] = all_biases[layer] + np.matmul(all_weights[layer], all_h[layer])
        all_h[layer+1] = ReLU(all_f[layer])
    all_f[K] = all_biases[K] + np.matmul(all_weights[K], all_h[K])
    net_output = all_f[K]
    return net_output, all_f, all_h


# -----------------------------
# Forward variance experiment
# -----------------------------
K = 50         # ✅ increased from 5 to 50 layers
D = 80         # ✅ increased from 8 to 80 hidden units
sigma_sq_omega = 2.0 / D   # ✅ He initialization to stabilize variance for ReLU

all_weights, all_biases = init_params(K, D, sigma_sq_omega)

n_data = 1000
data_in = np.random.normal(size=(1, n_data))
net_output, all_f, all_h = compute_network_output(data_in, all_weights, all_biases)

for layer in range(1, K + 1):
    print(f"Layer {layer}, std of hidden units = {np.std(all_h[layer]):.4f}")

# -----------------------------
# Loss and backward pass
# -----------------------------
def least_squares_loss(net_output, y):
    return np.sum((net_output - y) ** 2)

def d_loss_d_output(net_output, y):
    return 2 * (net_output - y)

def indicator_function(x):
    x_in = np.array(x)
    x_in[x_in >= 0] = 1
    x_in[x_in < 0] = 0
    return x_in

def backward_pass(all_weights, all_biases, all_f, all_h, y):
    K = len(all_weights) - 1
    all_dl_dweights = [None]*(K+1)
    all_dl_dbiases = [None]*(K+1)
    all_dl_df = [None]*(K+1)
    all_dl_dh = [None]*(K+1)
    all_dl_df[K] = d_loss_d_output(all_f[K], y)
    for layer in range(K, -1, -1):
        all_dl_dbiases[layer] = np.array(all_dl_df[layer])
        all_dl_dweights[layer] = np.matmul(all_dl_df[layer], all_h[layer].T)
        all_dl_dh[layer] = np.matmul(all_weights[layer].T, all_dl_df[layer])
        if layer > 0:
            all_dl_df[layer-1] = indicator_function(all_f[layer-1]) * all_dl_dh[layer]
    return all_dl_dweights, all_dl_dbiases, all_dl_dh, all_dl_df

# -----------------------------
# Backward gradient variance experiment
# -----------------------------
aggregate_dl_df = [None]*(K+1)
n_data = 100
for layer in range(1, K):
    aggregate_dl_df[layer] = np.zeros((D, n_data))

for c_data in range(n_data):
    data_in = np.random.normal(size=(1, 1))
    y = np.zeros((1, 1))
    net_output, all_f, all_h = compute_network_output(data_in, all_weights, all_biases)
    all_dl_dweights, all_dl_dbiases, all_dl_dh, all_dl_df = backward_pass(all_weights, all_biases, all_f, all_h, y)
    for layer in range(1, K):
        aggregate_dl_df[layer][:, c_data] = np.squeeze(all_dl_df[layer])

for layer in range(1, K):
    print(f"Layer {layer}, std of dl_df = {np.std(aggregate_dl_df[layer].ravel()):.4f}")
