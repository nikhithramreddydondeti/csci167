
# The purpose of this notebook is to understand what happens when we feed one neural network into another.

import numpy as np
import matplotlib.pyplot as plt

def ReLU(preactivation):
    activation = preactivation.clip(0.0)
    return activation

def shallow_1_1_3(x, activation_fn, phi_0,phi_1,phi_2,phi_3,
                  theta_10, theta_11, theta_20, theta_21, theta_30, theta_31):
    # Initial lines
    pre_1 = theta_10 + theta_11 * x
    pre_2 = theta_20 + theta_21 * x
    pre_3 = theta_30 + theta_31 * x
    # Activation functions
    act_1 = activation_fn(pre_1)
    act_2 = activation_fn(pre_2)
    act_3 = activation_fn(pre_3)
    # Weight activations
    w_act_1 = phi_1 * act_1
    w_act_2 = phi_2 * act_2
    w_act_3 = phi_3 * act_3
    # Combine weighted activation and add y offset
    y = phi_0 + w_act_1 + w_act_2 + w_act_3
    # Return everything we have calculated
    return y

def plot_neural_two_components(x_in, net1_out, net2_out, net12_out=None):
    fig, ax = plt.subplots(1,2)
    fig.set_size_inches(8.5, 8.5)
    fig.tight_layout(pad=3.0)
    ax[0].plot(x_in, net1_out,'r-')
    ax[0].set_xlabel('Net 1 input'); ax[0].set_ylabel('Net 1 output')
    ax[0].set_xlim([-1,1]); ax[0].set_ylim([-1,1])
    ax[0].set_aspect(1.0)
    ax[1].plot(x_in, net2_out,'b-')
    ax[1].set_xlabel('Net 2 input'); ax[1].set_ylabel('Net 2 output')
    ax[1].set_xlim([-1,1]); ax[1].set_ylim([-1,1])
    ax[1].set_aspect(1.0)
    plt.show()

    if net12_out is not None:
        fig, ax = plt.subplots()
        ax.plot(x_in, net12_out,'g-')
        ax.set_xlabel('Net 1 Input'); ax.set_ylabel('Net 2 Output')
        ax.set_xlim([-1,1]); ax.set_ylim([-1,1])
        ax.set_aspect(1.0)
        plt.show()

# Now let's define parameters
n1_theta_10 = 0.0   ; n1_theta_11 = -1.0
n1_theta_20 = 0     ; n1_theta_21 = 1.0
n1_theta_30 = -0.67 ; n1_theta_31 =  1.0
n1_phi_0 = 1.0; n1_phi_1 = -2.0; n1_phi_2 = -3.0; n1_phi_3 = 9.3

n2_theta_10 =  -0.6 ; n2_theta_11 = -1.0
n2_theta_20 =  0.2  ; n2_theta_21 = 1.0
n2_theta_30 =  -0.5  ; n2_theta_31 =  1.0
n2_phi_0 = 0.5; n2_phi_1 = -1.0; n2_phi_2 = -1.5; n2_phi_3 = 2.0

# Inputs
x = np.arange(-1,1,0.001)

# Run both networks
net1_out = shallow_1_1_3(x, ReLU,
                         n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3,
                         n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)

net2_out = shallow_1_1_3(x, ReLU,
                         n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3,
                         n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)

plot_neural_two_components(x, net1_out, net2_out)

# --------------------------------------------------------
# TODO 1: Feed the output of the first network into the second one
# Replace the placeholder with the actual composition.
# --------------------------------------------------------
net12_out = shallow_1_1_3(net1_out, ReLU,
                          n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3,
                          n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)

plot_neural_two_components(x, net1_out, net2_out, net12_out)

# --------------------------------------------------------
# Modify the second network (note the *-1 change for phi_1) and plot
# --------------------------------------------------------
net1_out = shallow_1_1_3(x, ReLU,
                         n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3,
                         n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)

net2_out = shallow_1_1_3(x, ReLU,
                         n2_phi_0, n2_phi_1*-1, n2_phi_2, n2_phi_3,
                         n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)

plot_neural_two_components(x, net1_out, net2_out)

# --------------------------------------------------------
# TODO 2: Feed net1 output into the modified second network
# --------------------------------------------------------
net12_out = shallow_1_1_3(net1_out, ReLU,
                          n2_phi_0, n2_phi_1*-1, n2_phi_2, n2_phi_3,
                          n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)

plot_neural_two_components(x, net1_out, net2_out, net12_out)

# --------------------------------------------------------
# Change the first network (scale phi_1 by 0.5) and plot
# --------------------------------------------------------
net1_out = shallow_1_1_3(x, ReLU,
                         n1_phi_0, n1_phi_1*0.5, n1_phi_2, n1_phi_3,
                         n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)

net2_out = shallow_1_1_3(x, ReLU,
                         n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3,
                         n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)

plot_neural_two_components(x, net1_out, net2_out)

# --------------------------------------------------------
# TODO 3: Feed modified net1 output into original net2
# --------------------------------------------------------
net12_out = shallow_1_1_3(net1_out, ReLU,
                          n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3,
                          n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)

plot_neural_two_components(x, net1_out, net2_out, net12_out)

# --------------------------------------------------------
# Make the networks identical (net2 is copy of net1) and plot
# --------------------------------------------------------
net1_out = shallow_1_1_3(x, ReLU,
                         n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3,
                         n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)

net2_out_new = shallow_1_1_3(x, ReLU,
                            n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3,
                            n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)

plot_neural_two_components(x, net1_out, net2_out_new)

# --------------------------------------------------------
# TODO 4: Feed net1 into an identical copy of itself
# --------------------------------------------------------
net12_out = shallow_1_1_3(net1_out, ReLU,
                          n1_phi_0, n1_phi_1, n1_phi_2, n1_phi_3,
                          n1_theta_10, n1_theta_11, n1_theta_20, n1_theta_21, n1_theta_30, n1_theta_31)

plot_neural_two_components(x, net1_out, net2_out_new, net12_out)

# --------------------------------------------------------
# TODO 5: Compose three networks: (first -> copy of first -> original second)
# Then plot net12_out, net2_out and net123_out
# --------------------------------------------------------
net123_out = shallow_1_1_3(net12_out, ReLU,
                           n2_phi_0, n2_phi_1, n2_phi_2, n2_phi_3,
                           n2_theta_10, n2_theta_11, n2_theta_20, n2_theta_21, n2_theta_30, n2_theta_31)

plot_neural_two_components(x, net12_out, net2_out, net123_out)

# --------------------------------------------------------
# Conceptual answers (printed for convenience)
# --------------------------------------------------------
print("Conceptual note:")
print(" - A shallow 1-input 1-output net with 3 ReLU hidden units can create up to 4 linear regions")
print(" - Composition (upper bound) multiplies region counts.")
print(" - If we chain N copies of the first network and then feed into the second network,")
print("   an (upper-bound) count of linear regions is (k1+1)^N * (k2+1).")
print("   With k1=3 hidden ReLUs and k2=3 hidden ReLUs, that is 4^N * 4 = 4^(N+1).")
print(" - Example: for N=2 (first -> copy -> second) upper bound = 4^(2+1) = 64 linear regions.")
